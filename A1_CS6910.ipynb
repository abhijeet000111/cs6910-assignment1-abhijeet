{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb41248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\om\\anaconda3\\lib\\site-packages (2.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\om\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b3d06",
   "metadata": {},
   "source": [
    "Import the Dataset:\n",
    "In your Python script or Jupyter Notebook, import the Fashion-MNIST dataset from Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61399bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca97b20",
   "metadata": {},
   "source": [
    "Load the Dataset:\n",
    "Use the load_data() function from the fashion_mnist module to load the dataset. This function returns training and testing data along with their corresponding labels:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4284f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dbd9ed",
   "metadata": {},
   "source": [
    "Explore the Dataset: We will explore the dataset to understand its structure and contents. We can check the shape of the training and testing data arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c7149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d9edf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data (images): (60000, 28, 28)\n",
      "Shape of testing data (images): (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of data\n",
    "print(\"Shape of training data (images):\", X_train.shape)\n",
    "print(\"Shape of testing data (images):\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c620d",
   "metadata": {},
   "source": [
    "Preprocessing: We will preprocess the data by normalizing the pixel values to the range [0, 1] and flattening the images to a 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb79b2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of flattened training data: (60000, 784)\n",
      "Shape of flattened testing data: (10000, 784)\n",
      "Number of features (dimension) of input data: 784\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# Normalize the pixel values to the range [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Flatten the images to a 1D array\n",
    "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test_flat = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "# Print the shape of flattened data\n",
    "print(\"Shape of flattened training data:\", X_train_flat.shape)\n",
    "print(\"Shape of flattened testing data:\", X_test_flat.shape)\n",
    "print(\"Number of features (dimension) of input data:\", X_train_flat.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955bb5d",
   "metadata": {},
   "source": [
    "Question 1 - Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset.\n",
    "\n",
    "Plot one sample image for each class in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab28e9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20776\\1969240367.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Plot one sample image for each class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Find index of the first occurrence of class i in y_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Define class labels\n",
    "class_labels = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\"\n",
    "}\n",
    "\n",
    "# Plot one sample image for each class\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 7))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Find index of the first occurrence of class i in y_train\n",
    "    idx = next(idx for idx, label in enumerate(y_train) if label == i)\n",
    "    ax.imshow(X_train[idx], cmap='gray')\n",
    "    ax.set_title(class_labels[i])\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75efd72",
   "metadata": {},
   "source": [
    "Question 2 (10 Marks)\n",
    "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\n",
    "\n",
    "Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf4b377",
   "metadata": {},
   "source": [
    "To implement a feedforward neural network for the Fashion-MNIST dataset, we'll create a class that represents the neural network. This class should allow for flexibility in terms of the number of hidden layers and neurons in each hidden layer. We'll use the sigmoid activation function for the hidden layers and the softmax activation function for the output layer to obtain a probability distribution over the 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711814ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FeedforwardNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layer_sizes, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = len(hidden_layer_sizes) + 1\n",
    "        self.weights, self.biases = self.initialize_weights_and_biases()\n",
    "\n",
    "    def initialize_weights_and_biases(self):\n",
    "        sizes = [self.input_size] + self.hidden_layer_sizes + [self.output_size]\n",
    "        weights = [np.random.randn(next_size, prev_size) for prev_size, next_size in zip(sizes[:-1], sizes[1:])]\n",
    "        biases = [np.random.randn(size, 1) for size in sizes[1:]]\n",
    "        return weights, biases\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=0))  # for numerical stability\n",
    "        return exp_z / np.sum(exp_z, axis=0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activations[-1]) + b\n",
    "            a = self.sigmoid(z) if w is not self.weights[-1] else self.softmax(z)\n",
    "            activations.append(a)\n",
    "        return activations\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ae393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_size = 784  # Number of input features (size of each image)\n",
    "hidden_layer_sizes = [64, 32]  # Number of neurons in each hidden layer\n",
    "output_size = 10  # Number of output classes\n",
    "\n",
    "# Initialize feedforward neural network\n",
    "model = FeedforwardNeuralNetwork(input_size, hidden_layer_sizes, output_size)\n",
    "\n",
    "# Generate random input data (batch size of 1 and 784 features)\n",
    "input_data = np.random.rand(input_size, 1)\n",
    "\n",
    "# Forward pass through the network\n",
    "output_probabilities = model.forward(input_data)\n",
    "\n",
    "# Display the output probabilities corresponding to each class\n",
    "class_labels = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\"\n",
    "}\n",
    "\n",
    "for i, prob in enumerate(output_probabilities[-1].flatten()):\n",
    "    print(f\"Probability of class {class_labels[i]}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9f1772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layer_sizes, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = len(hidden_layer_sizes) + 1\n",
    "        self.weights, self.biases = self.initialize_weights_and_biases()\n",
    "        # Variables for optimization algorithms\n",
    "        self.velocities_w = [np.zeros_like(w) for w in self.weights]\n",
    "        self.velocities_b = [np.zeros_like(b) for b in self.biases]\n",
    "        self.momentums_w = [np.zeros_like(w) for w in self.weights]\n",
    "        self.momentums_b = [np.zeros_like(b) for b in self.biases]\n",
    "        self.squared_gradients_w = [np.zeros_like(w) for w in self.weights]\n",
    "        self.squared_gradients_b = [np.zeros_like(b) for b in self.biases]\n",
    "        self.iterations = 0\n",
    "        \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        predictions = self.predict(X_test)\n",
    "        accuracy = np.mean(np.argmax(predictions, axis=0) == np.argmax(y_test, axis=0))\n",
    "        return accuracy\n",
    "    \n",
    "    def predict(self, X):\n",
    "        activations = self.forward(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def initialize_weights_and_biases(self):\n",
    "        sizes = [self.input_size] + self.hidden_layer_sizes + [self.output_size]\n",
    "        weights = [np.random.randn(next_size, prev_size) for prev_size, next_size in zip(sizes[:-1], sizes[1:])]\n",
    "        biases = [np.random.randn(size, 1) for size in sizes[1:]]\n",
    "        return weights, biases\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=0))  # for numerical stability\n",
    "        return exp_z / np.sum(exp_z, axis=0)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activations[-1]) + b\n",
    "            a = self.sigmoid(z) if w is not self.weights[-1] else self.softmax(z)\n",
    "            activations.append(a)\n",
    "        return activations\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[1]  # Number of samples\n",
    "        activations = self.forward(X)\n",
    "        deltas = [None] * self.num_layers\n",
    "        gradients = [None] * (self.num_layers - 1)\n",
    "        \n",
    "        # Compute error for output layer\n",
    "        deltas[-1] = activations[-1] - y\n",
    "        \n",
    "        # Backpropagate the error\n",
    "        for l in range(self.num_layers - 2, -1, -1):\n",
    "            gradients[l] = np.dot(deltas[l+1], activations[l].T) / m\n",
    "            deltas[l] = np.dot(self.weights[l+1].T, deltas[l+1]) * activations[l] * (1 - activations[l])\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients, learning_rate, optimizer):\n",
    "        self.iterations += 1  # Increment iteration count\n",
    "        if optimizer == 'sgd':\n",
    "            self.sgd_update(gradients, learning_rate)\n",
    "        elif optimizer == 'momentum':\n",
    "            self.momentum_update(gradients, learning_rate, momentum=0.9)\n",
    "        elif optimizer == 'nesterov':\n",
    "            self.nesterov_update(gradients, learning_rate, momentum=0.9)\n",
    "        elif optimizer == 'rmsprop':\n",
    "            self.rmsprop_update(gradients, learning_rate, beta=0.9, epsilon=1e-8)\n",
    "        elif optimizer == 'adam':\n",
    "            self.adam_update(gradients, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "        elif optimizer == 'nadam':\n",
    "            self.nadam_update(gradients, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown optimizer\")\n",
    "    \n",
    "    def sgd_update(self, gradients, learning_rate):\n",
    "        for l in range(self.num_layers - 1):\n",
    "            self.weights[l] -= learning_rate * gradients[l]\n",
    "            self.biases[l] -= learning_rate * np.mean(gradients[l], axis=1, keepdims=True)\n",
    "    \n",
    "    def momentum_update(self, gradients, learning_rate, momentum):\n",
    "        for l in range(self.num_layers - 1):\n",
    "            self.velocities_w[l] = momentum * self.velocities_w[l] - learning_rate * gradients[l]\n",
    "            self.velocities_b[l] = momentum * self.velocities_b[l] - learning_rate * np.mean(gradients[l], axis=1, keepdims=True)\n",
    "            self.weights[l] += self.velocities_w[l]\n",
    "            self.biases[l] += self.velocities_b[l]\n",
    "\n",
    "    def nesterov_update(self, gradients, learning_rate, momentum):\n",
    "        for l in range(self.num_layers - 1):\n",
    "            self.velocities_w[l] = momentum * self.velocities_w[l] - learning_rate * gradients[l]\n",
    "            self.velocities_b[l] = momentum * self.velocities_b[l] - learning_rate * np.mean(gradients[l], axis=1, keepdims=True)\n",
    "            self.weights[l] += momentum * self.velocities_w[l] - learning_rate * gradients[l]\n",
    "            self.biases[l] += momentum * self.velocities_b[l] - learning_rate * np.mean(gradients[l], axis=1, keepdims=True)\n",
    "\n",
    "    def rmsprop_update(self, gradients, learning_rate, beta, epsilon):\n",
    "        for l in range(self.num_layers - 1):\n",
    "            self.squared_gradients_w[l] = beta * self.squared_gradients_w[l] + (1 - beta) * np.square(gradients[l])\n",
    "            self.squared_gradients_b[l] = beta * self.squared_gradients_b[l] + (1 - beta) * np.square(np.mean(gradients[l], axis=1, keepdims=True))\n",
    "            self.weights[l] -= learning_rate * gradients[l] / (np.sqrt(self.squared_gradients_w[l]) + epsilon)\n",
    "            self.biases[l] -= learning_rate * np.mean(gradients[l], axis=1, keepdims=True) / (np.sqrt(self.squared_gradients_b[l]) + epsilon)\n",
    "\n",
    "    def adam_update(self, gradients, learning_rate, beta1, beta2, epsilon):\n",
    "        for l in range(self.num_layers - 1):\n",
    "            self.momentums_w[l] = beta1 * self.momentums_w[l] + (1 - beta1) * gradients[l]\n",
    "            self.velocities_w[l] = beta2 * self.velocities_w[l] + (1 - beta2) * np.square(gradients[l])\n",
    "            self.weights[l] -= learning_rate * self.momentums_w[l] / (np.sqrt(self.velocities_w[l]) + epsilon)\n",
    "            \n",
    "            self.momentums_b[l] = beta1 * self.momentums_b[l] + (1 - beta1) * np.mean(gradients[l], axis=1, keepdims=True)\n",
    "            self.velocities_b[l] = beta2 * self.velocities_b[l] + (1 - beta2) * np.square(np.mean(gradients[l], axis=1, keepdims=True))\n",
    "            self.biases[l] -= learning_rate * self.momentums_b[l] / (np.sqrt(self.velocities_b[l]) + epsilon)\n",
    "\n",
    "    def nadam_update(self, gradients, learning_rate, beta1, beta2, epsilon):\n",
    "        for l in range(self.num_layers - 1):\n",
    "            self.momentums_w[l] = beta1 * self.momentums_w[l] + (1 - beta1) * gradients[l]\n",
    "            self.velocities_w[l] = beta2 * self.velocities_w[l] + (1 - beta2) * np.square(gradients[l])\n",
    "            self.momentums_w_corrected = self.momentums_w[l] / (1 - np.power(beta1, self.iterations))\n",
    "            self.velocities_w_corrected = self.velocities_w[l] / (1 - np.power(beta2, self.iterations))\n",
    "            self.weights[l] -= learning_rate * (self.momentums_w_corrected / (np.sqrt(self.velocities_w_corrected) + epsilon))\n",
    "\n",
    "            self.momentums_b[l] = beta1 * self.momentums_b[l] + (1 - beta1) * np.mean(gradients[l], axis=1, keepdims=True)\n",
    "            self.velocities_b[l] = beta2 * self.velocities_b[l] + (1 - beta2) * np.square(np.mean(gradients[l], axis=1, keepdims=True))\n",
    "            self.momentums_b_corrected = self.momentums_b[l] / (1 - np.power(beta1, self.iterations))\n",
    "            self.velocities_b_corrected = self.velocities_b[l] / (1 - np.power(beta2, self.iterations))\n",
    "            self.biases[l] -= learning_rate * (self.momentums_b_corrected / (np.sqrt(self.velocities_b_corrected) + epsilon))\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs, batch_size, learning_rate, optimizer):\n",
    "        m = X_train.shape[1]  # Number of training samples\n",
    "        num_batches = m // batch_size\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(num_batches):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "                X_batch = X_train[:, start:end]\n",
    "                y_batch = y_train[:, start:end]\n",
    "                \n",
    "                gradients = self.backward(X_batch, y_batch)\n",
    "                self.update_parameters(gradients, learning_rate, optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "919be12d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (64,784) and (10000,784) not aligned: 784 (dim 1) != 10000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20776\\3393763114.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Make predictions on flattened data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_flat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20776\\3360426637.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20776\\3360426637.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (64,784) and (10000,784) not aligned: 784 (dim 1) != 10000 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 784  # Number of input features (size of each image)\n",
    "hidden_layer_sizes = [64, 32]  # Number of neurons in each hidden layer\n",
    "output_size = 10  # Number of output classes\n",
    "\n",
    "# Flatten the input data\n",
    "X_test_flat = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "# Make predictions on flattened data\n",
    "predictions = model.predict(X_test_flat)\n",
    "\n",
    "\n",
    "# Initialize feedforward neural network\n",
    "model = NeuralNetwork(input_size, hidden_layer_sizes, output_size)\n",
    "\n",
    "# Train the network\n",
    "model.train(X_train, y_train, epochs=10, batch_size=32, learning_rate=0.01, optimizer='sgd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75bb08b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (64,784) and (10000,28,28) not aligned: 784 (dim 1) != 28 (dim 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20776\\2166455289.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Evaluate the network on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test Accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Make predictions on new data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20776\\3360426637.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, X_test, y_test)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20776\\3360426637.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20776\\3360426637.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (64,784) and (10000,28,28) not aligned: 784 (dim 1) != 28 (dim 1)"
     ]
    }
   ],
   "source": [
    "# Evaluate the network on the test data\n",
    "accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions on new data\n",
    "new_data_predictions = model.predict(new_data)\n",
    "print(\"Predictions on new data:\", new_data_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32648947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc00177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
