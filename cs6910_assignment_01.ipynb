{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ka5eSRLZNxsa",
   "metadata": {
    "id": "ka5eSRLZNxsa"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Uw_NX1joN5Lx",
   "metadata": {
    "id": "Uw_NX1joN5Lx"
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "k6OoRSl4N61K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k6OoRSl4N61K",
    "outputId": "65554de5-bf69-42eb-8371-b2e20f0d1246"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabhijeet001\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "S147T8IvN6_Y",
   "metadata": {
    "id": "S147T8IvN6_Y"
   },
   "outputs": [],
   "source": [
    "# my_project = CS6910_ASSIGNMENT_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x0xzEgmWN7EY",
   "metadata": {
    "id": "x0xzEgmWN7EY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d004a8",
   "metadata": {
    "id": "e1d004a8"
   },
   "outputs": [],
   "source": [
    "# %%writefile activation.py\n",
    "# ACTIVATION_FUNCTION\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def relu(pre_activation_vector):\n",
    "    \"\"\"\n",
    "    Calculate the ReLU activation of a pre-activation vector.\n",
    "\n",
    "    Args:\n",
    "    pre_activation_vector (numpy.ndarray): The input vector.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The ReLU activated vector.\n",
    "    \"\"\"\n",
    "    post_act = np.copy(pre_activation_vector)\n",
    "    post_act[post_act < 0] = 0\n",
    "    return post_act\n",
    "\n",
    "def sigmoid(pre_activation_vector):\n",
    "    \"\"\"\n",
    "    Calculate the sigmoid of a pre-activation layer.\n",
    "\n",
    "    Args:\n",
    "    pre_activation_vector (numpy.ndarray): The input vector.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The activated vector.\n",
    "    \"\"\"\n",
    "    activated_vector = np.empty_like(pre_activation_vector)\n",
    "    for i, elem in np.ndenumerate(pre_activation_vector):\n",
    "        activated_vector[i] = sigmoid_element_wise(elem)\n",
    "    return activated_vector\n",
    "\n",
    "def softmax(pre_activation_vector):\n",
    "    \"\"\"\n",
    "    Calculate the softmax of a pre-activation vector.\n",
    "\n",
    "    Args:\n",
    "    pre_activation_vector (numpy.ndarray): The input vector.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The softmax output.\n",
    "    \"\"\"\n",
    "    post_act = np.copy(pre_activation_vector)\n",
    "    max_exponent = np.max(post_act)\n",
    "    post_act = np.exp(post_act - max_exponent)\n",
    "    post_act = post_act / np.sum(post_act)\n",
    "    return post_act\n",
    "\n",
    "def sigmoid_element_wise(vector_component):\n",
    "    \"\"\"\n",
    "    Calculate the sigmoid function value of a component of a vector.\n",
    "\n",
    "    Args:\n",
    "    vector_component (float): The input value.\n",
    "\n",
    "    Returns:\n",
    "    float: The sigmoid value of the input.\n",
    "    \"\"\"\n",
    "    if vector_component >= 0:\n",
    "        return 1 / (1 + math.exp(-vector_component))\n",
    "    else:\n",
    "        return math.exp(vector_component) / (math.exp(vector_component) + 1)\n",
    "\n",
    "\n",
    "\n",
    "def activation_function(pre_activation_vector, context):\n",
    "    \"\"\"\n",
    "    Apply the specified activation function to the pre-activation vector.\n",
    "\n",
    "    Args:\n",
    "    pre_activation_vector (numpy.ndarray): The input vector.\n",
    "    context (str): The context specifying the activation function ('softmax', 'sigmoid', 'tanh', 'relu').\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The activated vector.\n",
    "    \"\"\"\n",
    "    if context == 'softmax':\n",
    "        return softmax(pre_activation_vector)\n",
    "    elif context == 'sigmoid':\n",
    "        return sigmoid(pre_activation_vector)\n",
    "    elif context == 'tanh':\n",
    "        return np.copy(np.tanh(pre_activation_vector))\n",
    "    elif context == 'relu':\n",
    "        return relu(pre_activation_vector)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4g0zCT-tN5RC",
   "metadata": {
    "id": "4g0zCT-tN5RC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2fdd301",
   "metadata": {
    "id": "c2fdd301"
   },
   "outputs": [],
   "source": [
    "# %%writefile loss.py\n",
    "# LOSS_TYPE\n",
    "\n",
    "\"\"\"This file contains various methods for calculating loss functions.\"\"\"\n",
    "import numpy as np\n",
    "def squared_error(label, softmax_output):\n",
    "    \"\"\"\n",
    "    Calculate the squared error loss.\n",
    "\n",
    "    Args:\n",
    "    label (int): The true label index.\n",
    "    softmax_output (numpy.ndarray): The softmax output vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The squared error loss.\n",
    "    \"\"\"\n",
    "    true_vector = np.zeros_like(softmax_output)\n",
    "    true_vector[label] = 1\n",
    "    size = float(len(softmax_output))\n",
    "    return np.array([(np.linalg.norm(true_vector - softmax_output) ** 2) / size]).reshape((1, 1))\n",
    "\n",
    "def cross_entropy(label, softmax_output):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "    label (int): The true label index.\n",
    "    softmax_output (numpy.ndarray): The softmax output vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The cross-entropy loss.\n",
    "    \"\"\"\n",
    "    if softmax_output[label] < 1e-8:\n",
    "        return -np.log(1e-8)\n",
    "    return -np.log(softmax_output[label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123c2ef3",
   "metadata": {
    "id": "123c2ef3"
   },
   "outputs": [],
   "source": [
    "# %%writefile grad.py\n",
    "# GRADIENT\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def squared_error_grad(y_hat, label):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of squared error loss with respect to the output activations.\n",
    "\n",
    "    Args:\n",
    "    y_hat (numpy.ndarray): The predicted output vector.\n",
    "    label (int): The true label index.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The gradient of the loss with respect to the output activations.\n",
    "    \"\"\"\n",
    "    temp = np.copy(y_hat)\n",
    "    temp[label] -= 1\n",
    "    temp = 2 * temp / len(y_hat)\n",
    "    norm = np.linalg.norm(temp)\n",
    "    if norm > 100.0:\n",
    "        return temp * 100.0 / norm\n",
    "    else:\n",
    "        return temp\n",
    "    \n",
    "def cross_entropy_grad(y_hat, label):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of cross-entropy loss with respect to the output activations.\n",
    "\n",
    "    Args:\n",
    "    y_hat (numpy.ndarray): The predicted output vector.\n",
    "    label (int): The true label index.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The gradient of the loss with respect to the output activations.\n",
    "    \"\"\"\n",
    "    temp = np.zeros_like(y_hat)\n",
    "    if y_hat[label] < 1e-8:\n",
    "        y_hat[label] = 1e-8\n",
    "    temp[label] = -1 / (y_hat[label])\n",
    "    norm = np.linalg.norm(temp)\n",
    "    if norm > 100.0:\n",
    "        return temp * 100.0 / norm\n",
    "    else:\n",
    "        return temp\n",
    "\n",
    "\n",
    "def output_grad(y_hat, label, loss_type):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the specified loss function with respect to the output activations.\n",
    "\n",
    "    Args:\n",
    "    y_hat (numpy.ndarray): The predicted output vector.\n",
    "    label (int): The true label index.\n",
    "    loss_type (str): The type of loss function ('cross_entropy' or 'squared_error').\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The gradient of the loss with respect to the output activations.\n",
    "    \"\"\"\n",
    "    if loss_type == 'cross_entropy':\n",
    "        return cross_entropy_grad(y_hat, label)\n",
    "    elif loss_type == 'squared_error':\n",
    "        return squared_error_grad(y_hat, label)\n",
    "\n",
    "def last_grad(y_hat, label):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the last layer's activations with respect to the output activations.\n",
    "\n",
    "    Args:\n",
    "    y_hat (numpy.ndarray): The predicted output vector.\n",
    "    label (int): The true label index.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The gradient of the last layer's activations with respect to the output activations.\n",
    "    \"\"\"\n",
    "    temp = np.copy(y_hat)\n",
    "    temp[label] = temp[label] - 1\n",
    "    norm = np.linalg.norm(temp)\n",
    "    if norm > 100.0:\n",
    "        return temp * 100.0 / norm\n",
    "    else:\n",
    "        return temp\n",
    "\n",
    "def tanh_grad(post_activation):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the hyperbolic tangent activation function.\n",
    "\n",
    "    Args:\n",
    "    post_activation (numpy.ndarray): The post-activation vector.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The gradient of the hyperbolic tangent activation function.\n",
    "    \"\"\"\n",
    "    return 1 - np.power(post_activation, 2)\n",
    "\n",
    "def relu_grad(pre_activation_vector):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the ReLU activation function.\n",
    "\n",
    "    Args:\n",
    "    pre_activation_vector (numpy.ndarray): The pre-activation vector.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The gradient of the ReLU activation function.\n",
    "    \"\"\"\n",
    "    grad = np.copy(pre_activation_vector)\n",
    "    grad[grad >= 0] = 1\n",
    "    grad[grad < 0] = 0\n",
    "    return grad\n",
    "\n",
    "def sigmoid_grad(post_activation):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the sigmoid activation function.\n",
    "\n",
    "    Args:\n",
    "    post_activation (numpy.ndarray): The post-activation vector.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The gradient of the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return np.multiply(post_activation, 1 - post_activation)\n",
    "\n",
    "def w_grad(network, transient_gradient, layer, x):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the weights with respect to the pre-activation values.\n",
    "\n",
    "    Args:\n",
    "    network (list): The network configuration.\n",
    "    transient_gradient (dict): The transient gradients.\n",
    "    layer (int): The layer index.\n",
    "    x (numpy.ndarray): The input vector.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The gradient of the weights with respect to the pre-activation values.\n",
    "    \"\"\"\n",
    "    if layer == 0:\n",
    "        temp = transient_gradient[layer]['a'] @ x.transpose()\n",
    "    else:\n",
    "        temp = transient_gradient[layer]['a'] @ network[layer - 1]['h'].transpose()\n",
    "    norm = np.linalg.norm(temp)\n",
    "    if norm > 10000.0:\n",
    "        return temp * 10000.0 / norm\n",
    "    else:\n",
    "        return temp\n",
    "\n",
    "def a_grad(network, transient_gradient, layer):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the activation values with respect to the pre-activation values.\n",
    "\n",
    "    Args:\n",
    "    network (list): The network configuration.\n",
    "    transient_gradient (dict): The transient gradients.\n",
    "    layer (int): The layer index.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The gradient of the activation values with respect to the pre-activation values.\n",
    "    \"\"\"\n",
    "    if network[layer]['context'] == 'sigmoid':\n",
    "        active_grad_ = sigmoid_grad(network[layer]['h'])\n",
    "    elif network[layer]['context'] == 'tanh':\n",
    "        active_grad_ = tanh_grad(network[layer]['h'])\n",
    "    elif network[layer]['context'] == 'relu':\n",
    "        active_grad_ = relu_grad(network[layer]['a'])\n",
    "    temp = np.multiply(transient_gradient[layer]['h'], active_grad_)\n",
    "    norm = np.linalg.norm(temp)\n",
    "    if norm > 100.0:\n",
    "        return temp * 100.0 / norm\n",
    "    else:\n",
    "        return temp\n",
    "\n",
    "def h_grad(network, transient_gradient, layer):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the hidden layer activations with respect to the pre-activation values.\n",
    "\n",
    "    Args:\n",
    "    network (list): The network configuration.\n",
    "    transient_gradient (dict): The transient gradients.\n",
    "    layer (int): The layer index.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The gradient of the hidden layer activations with respect to the pre-activation values.\n",
    "    \"\"\"\n",
    "    network[layer]['weight'].transpose()\n",
    "    temp = network[layer + 1]['weight'].transpose() @ transient_gradient[layer + 1]['a']\n",
    "    norm = np.linalg.norm(temp)\n",
    "    if norm > 100.0:\n",
    "        return temp * 100.0 / norm\n",
    "    else:\n",
    "        return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e14cdd88",
   "metadata": {
    "id": "e14cdd88"
   },
   "outputs": [],
   "source": [
    "# %%writefile optimiser.py\n",
    "# OPTIMISER\n",
    "\n",
    "import sys\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SimpleGradientDescent:\n",
    "    \"\"\"\n",
    "    Class for simple gradient descent optimiser.\n",
    "\n",
    "    Attributes:\n",
    "    eta (float): Learning rate.\n",
    "    layers (int): Number of layers in the network.\n",
    "    weight_decay (float): Weight decay coefficient.\n",
    "    calls (int): Number of optimiser calls.\n",
    "    lrc (float): Learning rate controller.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta, layers, weight_decay=0.0):\n",
    "        self.eta = eta\n",
    "        self.layers = layers\n",
    "        self.calls = 1\n",
    "        self.lrc = 1.0\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def descent(self, network, gradient):\n",
    "        for i in range(self.layers):\n",
    "            network[i]['weight'] = network[i]['weight'] - ((self.eta / self.lrc) * gradient[i]['weight']) - (\n",
    "                        self.eta * self.weight_decay * network[i]['weight'])\n",
    "            network[i]['bias'] -= ((self.eta / self.lrc) * gradient[i]['bias'])\n",
    "        self.calls += 1\n",
    "        if self.calls % 10 == 0:\n",
    "            self.lrc += 1.0\n",
    "\n",
    "class MomentumGradientDescent:\n",
    "    \"\"\"\n",
    "    Class for Momentum gradient descent optimiser.\n",
    "\n",
    "    Attributes:\n",
    "    eta (float): Learning rate.\n",
    "    gamma (float): Momentum coefficient.\n",
    "    layers (int): Number of layers in the network.\n",
    "    weight_decay (float): Weight decay coefficient.\n",
    "    calls (int): Number of optimiser calls.\n",
    "    lrc (float): Learning rate controller.\n",
    "    momentum (dict): Historical momentum.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta, layers, gamma, weight_decay=0.0):\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.layers = layers\n",
    "        self.calls = 1\n",
    "        self.lrc = 1\n",
    "        self.momentum = None\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def descent(self, network, gradient):\n",
    "        gamma = min(1 - 2 ** (-1 - math.log((self.calls / 250.0) + 1, 2)), self.gamma)\n",
    "\n",
    "        if self.momentum is None:\n",
    "            self.momentum = copy.deepcopy(gradient)\n",
    "            for i in range(self.layers):\n",
    "                self.momentum[i]['weight'] = (self.eta / self.lrc) * gradient[i]['weight']\n",
    "                self.momentum[i]['bias'] = (self.eta / self.lrc) * gradient[i]['bias']\n",
    "        else:\n",
    "            for i in range(self.layers):\n",
    "                self.momentum[i]['weight'] = gamma * self.momentum[i]['weight'] + (self.eta / self.lrc) * gradient[i][\n",
    "                    'weight']\n",
    "                self.momentum[i]['bias'] = gamma * self.momentum[i]['bias'] + (self.eta / self.lrc) * gradient[i][\n",
    "                    'bias']\n",
    "        for i in range(self.layers):\n",
    "            network[i]['weight'] = network[i]['weight'] - self.momentum[i]['weight'] - (\n",
    "                        (self.eta / self.lrc) * self.weight_decay * network[i]['weight'])\n",
    "            network[i]['bias'] -= self.momentum[i]['bias']\n",
    "\n",
    "        self.calls += 1\n",
    "        if self.calls % 10 == 0:\n",
    "            self.lrc += 1.0\n",
    "\n",
    "class NAG:\n",
    "    \"\"\"\n",
    "    Class for Nesterov Accelerated Gradient (NAG) optimiser.\n",
    "\n",
    "    Attributes:\n",
    "    eta (float): Learning rate.\n",
    "    gamma (float): NAG coefficient.\n",
    "    layers (int): Number of layers in the network.\n",
    "    weight_decay (float): Weight decay coefficient.\n",
    "    calls (int): Number of optimiser calls.\n",
    "    momentum (dict): Historical momentum.\n",
    "    lrc (float): Learning rate controller.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta, layers, gamma, weight_decay=0.0):\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.layers = layers\n",
    "        self.calls = 1\n",
    "        self.momentum = None\n",
    "        self.lrc = 1.0\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def lookahead(self, network):\n",
    "        if self.momentum is None:\n",
    "            pass\n",
    "        else:\n",
    "            for i in range(self.layers):\n",
    "                network[i]['weight'] -= self.gamma * self.momentum[i]['weight']\n",
    "                network[i]['bias'] -= self.gamma * self.momentum[i]['bias']\n",
    "\n",
    "    def descent(self, network, gradient):\n",
    "        for i in range(self.layers):\n",
    "            network[i]['weight'] = network[i]['weight'] - ((self.eta / self.lrc) * gradient[i]['weight']) - (\n",
    "                        (self.eta / self.lrc) * self.weight_decay * network[i]['weight'])\n",
    "            network[i]['bias'] -= self.eta * gradient[i]['bias']\n",
    "\n",
    "        gamma = min(1 - 2 ** (-1 - math.log((self.calls / 250.0) + 1, 2)), self.gamma)\n",
    "\n",
    "        if self.momentum is None:\n",
    "            self.momentum = copy.deepcopy(gradient)\n",
    "            for i in range(self.layers):\n",
    "                self.momentum[i]['weight'] = (self.eta / self.lrc) * gradient[i]['weight']\n",
    "                self.momentum[i]['bias'] = (self.eta / self.lrc) * gradient[i]['bias']\n",
    "        else:\n",
    "            for i in range(self.layers):\n",
    "                self.momentum[i]['weight'] = gamma * self.momentum[i]['weight'] + ((self.eta / self.lrc) * gradient[i][\n",
    "                    'weight'])\n",
    "                self.momentum[i]['bias'] = gamma * self.momentum[i]['bias'] + ((self.eta / self.lrc) * gradient[i][\n",
    "                    'bias'])\n",
    "\n",
    "        self.calls += 1\n",
    "        if self.calls % 10 == 0:\n",
    "            self.lrc += 1.0\n",
    "\n",
    "class RMSProp:\n",
    "    \"\"\"\n",
    "    Class for RMSProp optimiser.\n",
    "\n",
    "    Attributes:\n",
    "    eta (float): Learning rate.\n",
    "    beta (float): Decay parameter for denominator.\n",
    "    layers (int): Number of layers in the network.\n",
    "    calls (int): Number of optimiser calls.\n",
    "    epsilon (float): Epsilon value.\n",
    "    update (dict): Update rule for RMSProp.\n",
    "    weight_decay (float): Weight decay coefficient.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta, layers, beta, weight_decay=0.0):\n",
    "        self.eta = eta\n",
    "        self.beta = beta\n",
    "        self.layers = layers\n",
    "        self.calls = 1\n",
    "        self.epsilon = 0.001\n",
    "        self.update = None\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def descent(self, network, gradient):\n",
    "        if self.update is None:\n",
    "            self.update = copy.deepcopy(gradient)\n",
    "            for i in range(self.layers):\n",
    "                self.update[i]['weight'] = (1 - self.beta) * (gradient[i]['weight']) ** 2\n",
    "                self.update[i]['bias'] = (1 - self.beta) * (gradient[i]['bias']) ** 2\n",
    "        else:\n",
    "            for i in range(self.layers):\n",
    "                self.update[i]['weight'] = self.beta * self.update[i]['weight'] + (1 - self.beta) * (gradient[i][\n",
    "                    'weight']) ** 2\n",
    "                self.update[i]['bias'] = self.beta * self.update[i]['bias'] + (1 - self.beta) * (\n",
    "                    gradient[i]['bias']) ** 2\n",
    "\n",
    "        for i in range(self.layers):\n",
    "            network[i]['weight'] = network[i]['weight'] - np.multiply(\n",
    "                (self.eta / np.sqrt(self.update[i]['weight'] + self.epsilon)),\n",
    "                gradient[i]['weight']) - self.weight_decay * network[i]['weight']\n",
    "            network[i]['bias'] = network[i]['bias'] - np.multiply(\n",
    "                (self.eta / np.sqrt(self.update[i]['bias'] + self.epsilon)), gradient[i]['bias'])\n",
    "\n",
    "        self.calls += 1\n",
    "\n",
    "class ADAM:\n",
    "    \"\"\"\n",
    "    Class for ADAM optimiser.\n",
    "\n",
    "    Attributes:\n",
    "    eta (float): Learning rate.\n",
    "    layers (int): Number of layers in the network.\n",
    "    weight_decay (float): Weight decay coefficient.\n",
    "    beta1 (float): Exponential decay rate for the first moment estimates.\n",
    "    beta2 (float): Exponential decay rate for the second moment estimates.\n",
    "    eps (float): Epsilon value.\n",
    "    calls (int): Number of optimiser calls.\n",
    "    momentum (dict): First moment vector.\n",
    "    t_momentum (dict): Biased corrected first moment vector.\n",
    "    second_momentum (dict): Second moment vector.\n",
    "    t_second_momentum (dict): Biased corrected second moment vector.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta, layers, weight_decay=0.0, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.eta = eta\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.layers = layers\n",
    "        self.calls = 1\n",
    "        self.momentum = None\n",
    "        self.t_momentum = None\n",
    "        self.second_momentum = None\n",
    "        self.t_second_momentum = None\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def descent(self, network, gradient):\n",
    "        if self.momentum is None:\n",
    "            self.momentum = copy.deepcopy(gradient)\n",
    "            self.second_momentum = copy.deepcopy(gradient)\n",
    "            for i in range(self.layers):\n",
    "                self.momentum[i]['weight'][:] = np.zeros_like(gradient[i]['weight'])\n",
    "                self.momentum[i]['bias'][:] = np.zeros_like(gradient[i]['bias'])\n",
    "                self.second_momentum[i]['weight'][:] = np.zeros_like(gradient[i]['weight'])\n",
    "                self.second_momentum[i]['bias'][:] = np.zeros_like(gradient[i]['bias'])\n",
    "            self.t_momentum = copy.deepcopy(self.momentum)\n",
    "            self.t_second_momentum = copy.deepcopy(self.second_momentum)\n",
    "\n",
    "        for i in range(self.layers):\n",
    "            self.momentum[i]['weight'] = self.beta1 * self.momentum[i]['weight'] + (1 - self.beta1) * gradient[i][\n",
    "                'weight']\n",
    "            self.momentum[i]['bias'] = self.beta1 * self.momentum[i]['bias'] + (1 - self.beta1) * gradient[i]['bias'\n",
    "            ]\n",
    "\n",
    "            self.second_momentum[i]['weight'] = self.beta2 * self.second_momentum[i]['weight'] + (\n",
    "                    1 - self.beta2) * np.power(gradient[i][\n",
    "                                                   'weight'], 2)\n",
    "            self.second_momentum[i]['bias'] = self.beta2 * self.second_momentum[i]['bias'] + (\n",
    "                    1 - self.beta2) * np.power(gradient[i]['bias'\n",
    "                                               ], 2)\n",
    "\n",
    "        for i in range(self.layers):\n",
    "            self.t_momentum[i]['weight'][:] = (1 / (1 - (self.beta1 ** self.calls))) * self.momentum[i]['weight']\n",
    "            self.t_momentum[i]['bias'][:] = (1 / (1 - (self.beta1 ** self.calls))) * self.momentum[i]['bias']\n",
    "\n",
    "            self.t_second_momentum[i]['weight'][:] = (1 / (1 - (self.beta2 ** self.calls))) * \\\n",
    "                                                     self.second_momentum[i][\n",
    "                                                         'weight']\n",
    "            self.t_second_momentum[i]['bias'][:] = (1 / (1 - (self.beta2 ** self.calls))) * self.second_momentum[i][\n",
    "                'bias']\n",
    "\n",
    "        for i in range(self.layers):\n",
    "            temp = np.sqrt(self.t_second_momentum[i]['weight'])\n",
    "            temp_eps = temp + self.eps\n",
    "            temp_inv = 1 / temp_eps\n",
    "            network[i]['weight'] = network[i]['weight'] - self.eta * (\n",
    "                np.multiply(temp_inv, self.t_momentum[i]['weight'])) - (\n",
    "                                               self.eta * self.weight_decay * network[i]['weight'])\n",
    "\n",
    "            temp = np.sqrt(self.t_second_momentum[i]['bias'])\n",
    "            temp_eps = temp + self.eps\n",
    "            temp_inv = 1 / temp_eps\n",
    "            network[i]['bias'] -= self.eta * np.multiply(temp_inv, self.t_momentum[i]['bias'])\n",
    "\n",
    "        self.calls += 1\n",
    "\n",
    "class NADAM:\n",
    "    \"\"\"\n",
    "    Class for Nesterov Accelerated ADAM (NADAM) optimiser.\n",
    "\n",
    "    Attributes:\n",
    "    eta (float): Learning rate.\n",
    "    layers (int): Number of layers in the network.\n",
    "    weight_decay (float): Weight decay coefficient.\n",
    "    beta1 (float): Exponential decay rate for the first moment estimates.\n",
    "    beta2 (float): Exponential decay rate for the second moment estimates.\n",
    "    eps (float): Epsilon value.\n",
    "    calls (int): Number of optimiser calls.\n",
    "    momentum (dict): First moment vector.\n",
    "    second_momentum (dict): Second moment vector.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta, layers, weight_decay=0.0, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.eta = eta\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.layers = layers\n",
    "        self.calls = 1\n",
    "        self.momentum = None\n",
    "        self.second_momentum = None\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def descent(self, network, gradient):\n",
    "        if self.momentum is None:\n",
    "            self.momentum = copy.deepcopy(gradient)\n",
    "            self.second_momentum = copy.deepcopy(gradient)\n",
    "            for i in range(self.layers):\n",
    "                self.momentum[i]['weight'] = (1 - self.beta1) * gradient[i]['weight']\n",
    "                self.momentum[i]['bias'] = (1 - self.beta1) * gradient[i]['bias']\n",
    "                self.second_momentum[i]['weight'] = (1 - self.beta2) * np.power(gradient[i]['weight'], 2)\n",
    "                self.second_momentum[i]['bias'] = (1 - self.beta2) * np.power(gradient[i]['bias'], 2)\n",
    "        else:\n",
    "            for i in range(self.layers):\n",
    "                self.momentum[i]['weight'] = self.beta1 * self.momentum[i]['weight'] + (1 - self.beta1) * \\\n",
    "                                             gradient[i][\n",
    "                                                 'weight']\n",
    "                self.momentum[i]['bias'] = self.beta1 * self.momentum[i]['bias'] + (1 - self.beta1) * gradient[i][\n",
    "                    'bias'\n",
    "                ]\n",
    "                self.second_momentum[i]['weight'] = self.beta2 * self.second_momentum[i]['weight'] + (\n",
    "                        1 - self.beta2) * np.power(gradient[i][\n",
    "                                                       'weight'], 2)\n",
    "                self.second_momentum[i]['bias'] = self.beta2 * self.second_momentum[i]['bias'] + (\n",
    "                        1 - self.beta2) * np.power(gradient[i]['bias'\n",
    "                                                   ], 2)\n",
    "\n",
    "        m_t_hat = copy.deepcopy(self.momentum)\n",
    "        v_t_hat = copy.deepcopy(self.second_momentum)\n",
    "        for i in range(self.layers):\n",
    "            m_t_hat[i]['weight'] = (self.beta1 / (1 - (self.beta1 ** self.calls))) * self.momentum[i][\n",
    "                'weight'] + ((1 - self.beta1) / (1 - (self.beta1 ** self.calls))) * gradient[i]['weight']\n",
    "            m_t_hat[i]['bias'] = (self.beta1 / (1 - (self.beta1 ** self.calls))) * self.momentum[i]['bias'] + (\n",
    "                    (1 - self.beta1) / (1 - (self.beta1 ** self.calls))) * gradient[i]['bias']\n",
    "\n",
    "            v_t_hat[i]['weight'] = (self.beta2 / (1 - (self.beta2 ** self.calls))) * \\\n",
    "                                   self.second_momentum[i][\n",
    "                                       'weight']\n",
    "            v_t_hat[i]['bias'] = (self.beta2 / (1 - (self.beta2 ** self.calls))) * self.second_momentum[i][\n",
    "                'bias']\n",
    "\n",
    "        for i in range(self.layers):\n",
    "            temp = np.sqrt(self.second_momentum[i]['weight'] + self.eps)\n",
    "            temp_inv = 1 / temp\n",
    "            network[i]['weight'] = network[i]['weight'] - self.eta * (\n",
    "                np.multiply(temp_inv, m_t_hat[i]['weight'])) - (self.eta * self.weight_decay * network[i]['weight'])\n",
    "\n",
    "            temp = np.sqrt(self.second_momentum[i]['bias']) + self.eps\n",
    "            temp_inv = 1 / temp\n",
    "            network[i]['bias'] -= self.eta * np.multiply(temp_inv, v_t_hat[i]['bias'])\n",
    "\n",
    "        self.calls += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c43a22c0",
   "metadata": {
    "id": "c43a22c0"
   },
   "outputs": [],
   "source": [
    "# ############################################################################################################################\n",
    "# # %%writefile main.py\n",
    "# # MAIN\n",
    "\n",
    "# \"\"\"Implement Feed Forward neural network where the parameters are\n",
    "#    number of hidden layers and number of neurons in each hidden layer\"\"\"\n",
    "# # from loss import *\n",
    "# # from grad import *\n",
    "# # from activation import *\n",
    "# # from optimiser import *\n",
    "# import copy\n",
    "# from keras.datasets import fashion_mnist\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# import numpy as np\n",
    "# import wandb\n",
    "\n",
    "# \"\"\" get training and testing vectors\n",
    "#     Number of Training Images = 60000\n",
    "#     Number of Testing Images = 10000 \"\"\"\n",
    "# (trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
    "\n",
    "# last = 2\n",
    "# # network is a list of all the learning parameters in every layer and gradient is its copy\n",
    "# network = []\n",
    "# gradient = []\n",
    "# # store gradient w.r.t a single datapoint\n",
    "# transient_gradient = []\n",
    "# # will contain the total amount of loss for each timestep(1). One timestep is defined as one update of the parameters.\n",
    "# loss = 0\n",
    "\n",
    "\n",
    "# def forward_propagation(n, x):\n",
    "#     for i in range(n):\n",
    "#         if i == 0:\n",
    "#             network[i]['a'] = network[i]['weight'] @ x + network[i]['bias']\n",
    "#         else:\n",
    "#             network[i]['a'] = network[i]['weight'] @ network[i - 1]['h'] + network[i]['bias']\n",
    "\n",
    "#         network[i]['h'] = activation_function(network[i]['a'], context=network[i]['context'])\n",
    "\n",
    "\n",
    "# def backward_propagation(number_of_layers, x, y, number_of_datapoint, loss_type, clean=False):\n",
    "#     transient_gradient[number_of_layers - 1]['h'] = output_grad(network[number_of_layers - 1]['h'], y,\n",
    "#                                                                 loss_type=loss_type)\n",
    "#     transient_gradient[number_of_layers - 1]['a'] = last_grad(network[number_of_layers - 1]['h'], y)\n",
    "#     for i in range(number_of_layers - 2, -1, -1):\n",
    "#         transient_gradient[i]['h'] = h_grad(network=network, transient_gradient=transient_gradient, layer=i)\n",
    "#         transient_gradient[i]['a'] = a_grad(network=network, transient_gradient=transient_gradient, layer=i)\n",
    "#     for i in range(number_of_layers - 1, -1, -1):\n",
    "#         transient_gradient[i]['weight'] = w_grad(network=network, transient_gradient=transient_gradient, layer=i, x=x)\n",
    "#         transient_gradient[i]['bias'] = gradient[i]['a']\n",
    "#     if clean:\n",
    "#         gradient[number_of_layers - 1]['h'] = transient_gradient[number_of_layers - 1]['h'] / float(number_of_datapoint)\n",
    "#         gradient[number_of_layers - 1]['a'] = transient_gradient[number_of_layers - 1]['a'] / float(number_of_datapoint)\n",
    "#         for i in range(number_of_layers - 2, -1, -1):\n",
    "#             gradient[i]['h'] = transient_gradient[i]['h'] / float(number_of_datapoint)\n",
    "#             gradient[i]['a'] = transient_gradient[i]['a'] / float(number_of_datapoint)\n",
    "#         for i in range(number_of_layers - 1, -1, -1):\n",
    "#             gradient[i]['weight'] = transient_gradient[i]['weight'] / float(number_of_datapoint)\n",
    "#             gradient[i]['bias'] = transient_gradient[i]['bias'] / float(number_of_datapoint)\n",
    "#     else:\n",
    "\n",
    "#         gradient[number_of_layers - 1]['h'] += transient_gradient[number_of_layers - 1]['h'] / float(\n",
    "#             number_of_datapoint)\n",
    "#         gradient[number_of_layers - 1]['a'] += transient_gradient[number_of_layers - 1]['a'] / float(\n",
    "#             number_of_datapoint)\n",
    "#         for i in range(number_of_layers - 2, -1, -1):\n",
    "#             gradient[i]['h'] += transient_gradient[i]['h'] / float(number_of_datapoint)\n",
    "#             gradient[i]['a'] += transient_gradient[i]['a'] / float(number_of_datapoint)\n",
    "#         for i in range(number_of_layers - 1, -1, -1):\n",
    "#             gradient[i]['weight'] += transient_gradient[i]['weight'] / float(number_of_datapoint)\n",
    "#             gradient[i]['bias'] += transient_gradient[i]['bias'] / float(number_of_datapoint)\n",
    "\n",
    "# def augment_my_data(datapoints, labels, d, newSize):\n",
    "#     dataGenerator = ImageDataGenerator(rotation_range=15, shear_range=0.1, zoom_range=0.2, width_shift_range=0.1,\n",
    "#                                        height_shift_range=0.1, horizontal_flip=True, fill_mode='nearest')\n",
    "#     new_data = []\n",
    "#     new_label = []\n",
    "#     datapoints = datapoints.reshape((d, 28, 28, 1))\n",
    "#     i = 0\n",
    "#     for (data, label) in dataGenerator.flow(datapoints, labels, batch_size=1):\n",
    "#         new_data.append(data.reshape(28, 28))\n",
    "#         new_label.append(label)\n",
    "#         i += 1\n",
    "#         if i > newSize:\n",
    "#             break\n",
    "\n",
    "#     return np.array(new_data), np.array(new_label), newSize\n",
    "            \n",
    "            \n",
    "# # this function is used for validation, useful during hyperparameter tuning or model change.\n",
    "# def validate(number_of_layer, validateX, validateY, loss_type):\n",
    "#     loss_local = 0\n",
    "#     acc = 0\n",
    "#     if loss_type == 'cross_entropy':\n",
    "#         for x, y in zip(validateX, validateY):\n",
    "#             forward_propagation(number_of_layer, x.reshape(784, 1) / 255.0)\n",
    "#             # adding loss w.r.t to a single datapoint\n",
    "#             loss_local += cross_entropy(label=y, softmax_output=network[number_of_layer - 1]['h'])\n",
    "#             max_prob = np.argmax(network[number_of_layer - 1]['h'])\n",
    "#             if max_prob == y:\n",
    "#                 acc += 1\n",
    "#     elif loss_type == 'squared_error':\n",
    "#         for x, y in zip(validateX, validateY):\n",
    "#             forward_propagation(number_of_layer, x.reshape(784, 1) / 255.0)\n",
    "#             # adding loss w.r.t to a single datapoint\n",
    "#             loss_local += squared_error(label=y, softmax_output=network[number_of_layer - 1]['h'])\n",
    "#             max_prob = np.argmax(network[number_of_layer - 1]['h'])\n",
    "#             if max_prob == y:\n",
    "#                 acc += 1\n",
    "#     average_loss = loss_local / float(len(validateX))\n",
    "#     acc = acc / float(len(validateX))\n",
    "#     return [average_loss, acc]\n",
    "\n",
    "\n",
    "# # 1 epoch = 1 pass over the data\n",
    "# def fit(datapoints, batch, epochs, labels, opt, loss_type, augment):\n",
    "#     n = len(network)  # number of layers\n",
    "#     d = len(datapoints)  # number of data points\n",
    "#     \"\"\"        \n",
    "#     This variable will be employed to distinguish the training and validation sets:\n",
    "#     1. To follow the question's suggestion, we reserve 10% of the data: int(d * .1).\n",
    "#     2. Additionally, any remaining data is appended to the validation set to ensure that the training data can be evenly divided by the batch size: ((d - int(d * .1)) % batch).\n",
    "#     \"\"\"\n",
    "#     border = d - ((d - int(d * .1)) % batch + int(d * .1))\n",
    "#     # separating the validation data\n",
    "#     validateX = datapoints[border:]\n",
    "#     validateY = labels[border:]\n",
    "#     # deleting copied datapoints\n",
    "#     datapoints = datapoints[:border]\n",
    "#     labels = labels[:border]\n",
    "#     # updating d\n",
    "#     d = border\n",
    "#     # augmenting my datapoints\n",
    "#     if augment is not None:\n",
    "#         (datapoints, labels, d) = augment_my_data(datapoints=datapoints, labels=labels, d=d, newSize=d + augment * batch)\n",
    "\n",
    "#     # is used to stochastically select our data.\n",
    "#     shuffler = np.arange(0, d)\n",
    "#     # creating simple gradient descent optimiser\n",
    "\n",
    "#     # loop for epoch iteration\n",
    "#     for k in range(epochs):\n",
    "#         # iteration for different starting point for epoch\n",
    "#         # shuffler at the start of each epoch\n",
    "#         np.random.shuffle(shuffler)\n",
    "#         for i in range(0, d - batch + 1, batch):\n",
    "#             clean = True\n",
    "#             # initiating loss for current epoch\n",
    "#             global loss\n",
    "#             loss = 0\n",
    "#             if isinstance(opt, NAG):\n",
    "#                 opt.lookahead(network=network)\n",
    "#             # iterate over a batch\n",
    "#             for j in range(i, i + batch, 1):\n",
    "#                 # creating a single data vector and normalising color values between 0 to 1\n",
    "#                 x = datapoints[shuffler[j]].reshape(784, 1) / 255.0\n",
    "#                 y = labels[shuffler[j]]\n",
    "#                 forward_propagation(n, x)\n",
    "\n",
    "#                 backward_propagation(n, x, y, number_of_datapoint=batch, loss_type=loss_type, clean=clean)\n",
    "#                 clean = False\n",
    "\n",
    "#             opt.descent(network=network, gradient=gradient)\n",
    "\n",
    "#         # for wandb logging\n",
    "#         validation_result = validate(number_of_layer=n, validateX=validateX, validateY=validateY,\n",
    "#                                      loss_type=loss_type)\n",
    "#         training_result = validate(number_of_layer=n, validateX=datapoints,\n",
    "#                                    validateY=labels, loss_type=loss_type)\n",
    "\n",
    "#         # printing average loss.\n",
    "#         wandb.log({\"val_accuracy\": validation_result[1], 'val_loss': validation_result[0][0],\n",
    "#                    'train_accuracy': training_result[1], 'train_loss': training_result[0][0], 'epochs': k + 1})  # EPOCH -> EPOCHES\n",
    "\n",
    "#         if np.isnan(validation_result[0])[0]:\n",
    "#             return\n",
    "\n",
    "\n",
    "# \"\"\" Adding a specific layer on top of the previous one, the layers are constructed incrementally. \n",
    "#     The context determines the type of layer we have, such as Sigmoid or Tanh. \n",
    "#     When any number is passed to input_dim, it is considered as the first layer.\n",
    "#  \"\"\"\n",
    "\n",
    "\n",
    "# def add_layer(number_of_neurons, context, weight_init, input_dim=None):\n",
    "#     # Initialize an Empty Dictionary: layer\n",
    "#     layer = {}\n",
    "#     if weight_init == 'random':\n",
    "#         if input_dim is not None:\n",
    "#             layer['weight'] = np.random.rand(number_of_neurons, input_dim)\n",
    "#         else:\n",
    "#             # get number of neurons in the previous layer\n",
    "#             previous_lay_neuron_num = network[-1]['h'].shape[0]\n",
    "#             layer['weight'] = np.random.rand(number_of_neurons, previous_lay_neuron_num)\n",
    "\n",
    "#     elif weight_init == 'xavier':\n",
    "#         if input_dim is not None:\n",
    "#             layer['weight'] = np.random.normal(size=(number_of_neurons, input_dim))\n",
    "#             xavier = input_dim\n",
    "#         else:\n",
    "#             # get number of neurons in the previous layer\n",
    "#             previous_lay_neuron_num = network[-1]['h'].shape[0]\n",
    "#             layer['weight'] = np.random.normal(size=(number_of_neurons, previous_lay_neuron_num))\n",
    "#             xavier = previous_lay_neuron_num\n",
    "#         if context == 'relu':\n",
    "#             # relu has different optimal weight initialization.\n",
    "#             layer['weight'] = layer['weight'] * math.sqrt(2 / float(xavier))\n",
    "#         else:\n",
    "#             layer['weight'] = layer['weight'] * math.sqrt(1 / float(xavier))\n",
    "#     # initialise a 1-D array of size n with random samples from a uniform distribution over [0, 1).\n",
    "#     layer['bias'] = np.zeros((number_of_neurons, 1))\n",
    "#     # initialises a 2-D array of size [n*1] and type float with element having value as 1.\n",
    "#     layer['h'] = np.zeros((number_of_neurons, 1))\n",
    "#     layer['a'] = np.zeros((number_of_neurons, 1))\n",
    "#     layer['context'] = context\n",
    "#     network.append(layer)\n",
    "\n",
    "\n",
    "# \"\"\"master() is used to initialise all the learning parameters in every layer and then start the training process\"\"\"\n",
    "\n",
    "# def master(batch, epochs, output_dim, activation, opt, layer_1, layer_2, layer_3, weight_init='xavier',loss_type='cross_entropy',\n",
    "#            augment=None):\n",
    "    \n",
    "#     \"\"\"initializing number of input features per datapoint as 784, since dataset consists of 28x28 pixel grayscale images\n",
    "#        :param augment: \"\"\"\n",
    "    \n",
    "#     n_features = 784\n",
    "#     global network\n",
    "#     global gradient\n",
    "#     global transient_gradient\n",
    "#     network = []\n",
    "#     gradient = []\n",
    "#     transient_gradient = []\n",
    "#     # adding layers\n",
    "#     add_layer(number_of_neurons=layer_1, context=activation, input_dim=784, weight_init=weight_init)\n",
    "#     # creating hidden layers\n",
    "#     add_layer(number_of_neurons=layer_2, context=activation, weight_init=weight_init)\n",
    "#     add_layer(number_of_neurons=layer_3, context=activation, weight_init=weight_init)\n",
    "#     add_layer(number_of_neurons=output_dim, context='softmax', weight_init=weight_init)\n",
    "\n",
    "#     \"\"\"Copying the structure of network.\"\"\"\n",
    "#     gradient = copy.deepcopy(network)\n",
    "#     transient_gradient = copy.deepcopy(network)\n",
    "#     fit(datapoints=trainX, labels=trainY, batch=batch, epochs=epochs, opt=opt,\n",
    "#         loss_type=loss_type,augment=augment)\n",
    "#     return network\n",
    "\n",
    "\n",
    "# def train():\n",
    "#     run = wandb.init()\n",
    "#     opti = None\n",
    "    \n",
    "#     # Overwrite the random run names chosen by wandb\n",
    "    \n",
    "#     name_str = f'act:{run.config.activation}_opt:{run.config.optimiser}_bs:{run.config.batch_size}_winit:{run.config.weight_init}_epoc:{run.config.epochs}_numlayers:{run.config.num_layers}_l1:{run.config.layer_1}_l2:{run.config.layer_2}_l3:{run.config.layer_3}_lr:{run.config.learning_rate}_wtDec:{run.config.weight_decay}'\n",
    "\n",
    "#     run.name = name_str\n",
    "\n",
    "#     if run.config.optimiser == 'nag':\n",
    "#         opti = NAG(layers=4, eta=run.config.learning_rate, gamma=.90, weight_decay=run.config.weight_decay)\n",
    "#     elif run.config.optimiser == 'rmsprop':\n",
    "#         opti = RMSProp(layers=4, eta=run.config.learning_rate, beta=.90, weight_decay=run.config.weight_decay)\n",
    "#     elif run.config.optimiser == 'sgd':\n",
    "#         opti = SimpleGradientDescent(layers=4, eta=run.config.learning_rate, weight_decay=run.config.weight_decay)\n",
    "#     elif run.config.optimiser == 'mom':\n",
    "#         opti = MomentumGradientDescent(layers=4, eta=run.config.learning_rate, gamma=.99,\n",
    "#                                        weight_decay=run.config.weight_decay)\n",
    "#     elif run.config.optimiser == 'adam':\n",
    "#         opti = ADAM(layers=4, eta=run.config.learning_rate, weight_decay=run.config.weight_decay)\n",
    "#     elif run.config.optimiser == 'nadam':\n",
    "#         opti = NADAM(layers=4, eta=run.config.learning_rate, weight_decay=run.config.weight_decay)\n",
    "\n",
    "#     master(epochs=run.config.epochs, batch=run.config.batch_size, output_dim=10,\n",
    "#            opt=opti, weight_init=run.config.weight_init, activation=run.config.activation, layer_1=run.config.layer_1,\n",
    "#            layer_3=run.config.layer_3, layer_2=run.config.layer_2, loss_type=run.config.loss_type, augment=100)\n",
    "\n",
    "#     # exit intt # abhi\n",
    "#     wandb.finish()\n",
    "\n",
    "\n",
    "############################################################################################################################\n",
    "# %%writefile main.py\n",
    "# MAIN\n",
    "\n",
    "\"\"\"Implement Feed Forward neural network where the parameters are\n",
    "   number of hidden layers and number of neurons in each hidden layer\"\"\"\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import wandb\n",
    "\n",
    "# Load fashion MNIST dataset\n",
    "(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
    "\n",
    "last = 2\n",
    "network = []  # List to store learning parameters in every layer\n",
    "gradient = []  # List to store gradients\n",
    "transient_gradient = []  # List to store gradients w.r.t a single datapoint\n",
    "loss = 0  # Variable to store total loss\n",
    "\n",
    "def forward_propagation(n, x):\n",
    "    \"\"\"\n",
    "    Perform forward propagation through the neural network.\n",
    "\n",
    "    Args:\n",
    "        n (int): Number of layers in the network.\n",
    "        x (numpy.ndarray): Input data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Iterate over layers\n",
    "    for i in range(n):\n",
    "        if i == 0:\n",
    "            network[i]['a'] = network[i]['weight'] @ x + network[i]['bias']\n",
    "        else:\n",
    "            network[i]['a'] = network[i]['weight'] @ network[i - 1]['h'] + network[i]['bias']\n",
    "\n",
    "        network[i]['h'] = activation_function(network[i]['a'], context=network[i]['context'])\n",
    "\n",
    "def backward_propagation(number_of_layers, x, y, number_of_datapoint, loss_type, clean=False):\n",
    "    \"\"\"\n",
    "    Perform backward propagation through the neural network.\n",
    "\n",
    "    Args:\n",
    "        number_of_layers (int): Number of layers in the network.\n",
    "        x (numpy.ndarray): Input data.\n",
    "        y (int): True label.\n",
    "        number_of_datapoint (int): Number of datapoints.\n",
    "        loss_type (str): Type of loss function.\n",
    "        clean (bool, optional): Whether to clean the gradients. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    transient_gradient[number_of_layers - 1]['h'] = output_grad(network[number_of_layers - 1]['h'], y,\n",
    "                                                                loss_type=loss_type)\n",
    "    transient_gradient[number_of_layers - 1]['a'] = last_grad(network[number_of_layers - 1]['h'], y)\n",
    "    for i in range(number_of_layers - 2, -1, -1):\n",
    "        transient_gradient[i]['h'] = h_grad(network=network, transient_gradient=transient_gradient, layer=i)\n",
    "        transient_gradient[i]['a'] = a_grad(network=network, transient_gradient=transient_gradient, layer=i)\n",
    "    for i in range(number_of_layers - 1, -1, -1):\n",
    "        transient_gradient[i]['weight'] = w_grad(network=network, transient_gradient=transient_gradient, layer=i, x=x)\n",
    "        transient_gradient[i]['bias'] = gradient[i]['a']\n",
    "    if clean:\n",
    "        gradient[number_of_layers - 1]['h'] = transient_gradient[number_of_layers - 1]['h'] / float(number_of_datapoint)\n",
    "        gradient[number_of_layers - 1]['a'] = transient_gradient[number_of_layers - 1]['a'] / float(number_of_datapoint)\n",
    "        for i in range(number_of_layers - 2, -1, -1):\n",
    "            gradient[i]['h'] = transient_gradient[i]['h'] / float(number_of_datapoint)\n",
    "            gradient[i]['a'] = transient_gradient[i]['a'] / float(number_of_datapoint)\n",
    "        for i in range(number_of_layers - 1, -1, -1):\n",
    "            gradient[i]['weight'] = transient_gradient[i]['weight'] / float(number_of_datapoint)\n",
    "            gradient[i]['bias'] = transient_gradient[i]['bias'] / float(number_of_datapoint)\n",
    "    else:\n",
    "        gradient[number_of_layers - 1]['h'] += transient_gradient[number_of_layers - 1]['h'] / float(\n",
    "            number_of_datapoint)\n",
    "        gradient[number_of_layers - 1]['a'] += transient_gradient[number_of_layers - 1]['a'] / float(\n",
    "            number_of_datapoint)\n",
    "        for i in range(number_of_layers - 2, -1, -1):\n",
    "            gradient[i]['h'] += transient_gradient[i]['h'] / float(number_of_datapoint)\n",
    "            gradient[i]['a'] += transient_gradient[i]['a'] / float(number_of_datapoint)\n",
    "        for i in range(number_of_layers - 1, -1, -1):\n",
    "            gradient[i]['weight'] += transient_gradient[i]['weight'] / float(number_of_datapoint)\n",
    "            gradient[i]['bias'] += transient_gradient[i]['bias'] / float(number_of_datapoint)\n",
    "\n",
    "def augment_my_data(datapoints, labels, d, newSize):\n",
    "    \"\"\"\n",
    "    Augment dataset with additional samples.\n",
    "\n",
    "    Args:\n",
    "        datapoints (numpy.ndarray): Input data.\n",
    "        labels (numpy.ndarray): Labels.\n",
    "        d (int): Number of data points.\n",
    "        newSize (int): New size of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Augmented data.\n",
    "        numpy.ndarray: Augmented labels.\n",
    "        int: Updated size of the dataset.\n",
    "    \"\"\"\n",
    "    dataGenerator = ImageDataGenerator(rotation_range=15, shear_range=0.1, zoom_range=0.2, width_shift_range=0.1,\n",
    "                                       height_shift_range=0.1, horizontal_flip=True, fill_mode='nearest')\n",
    "    new_data = []\n",
    "    new_label = []\n",
    "    datapoints = datapoints.reshape((d, 28, 28, 1))\n",
    "    i = 0\n",
    "    for (data, label) in dataGenerator.flow(datapoints, labels, batch_size=1):\n",
    "        new_data.append(data.reshape(28, 28))\n",
    "        new_label.append(label)\n",
    "        i += 1\n",
    "        if i > newSize:\n",
    "            break\n",
    "\n",
    "    return np.array(new_data), np.array(new_label), newSize\n",
    "\n",
    "def validate(number_of_layer, validateX, validateY, loss_type):\n",
    "    \"\"\"\n",
    "    Validate the neural network.\n",
    "\n",
    "    Args:\n",
    "        number_of_layer (int): Number of layers in the network.\n",
    "        validateX (numpy.ndarray): Validation input data.\n",
    "        validateY (numpy.ndarray): Validation labels.\n",
    "        loss_type (str): Type of loss function.\n",
    "\n",
    "    Returns:\n",
    "        list: Validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    loss_local = 0\n",
    "    acc = 0\n",
    "    if loss_type == 'cross_entropy':\n",
    "        for x, y in zip(validateX, validateY):\n",
    "            forward_propagation(number_of_layer, x.reshape(784, 1) / 255.0)\n",
    "            loss_local += cross_entropy(label=y, softmax_output=network[number_of_layer - 1]['h'])\n",
    "            max_prob = np.argmax(network[number_of_layer - 1]['h'])\n",
    "            if max_prob == y:\n",
    "                acc += 1\n",
    "    elif loss_type == 'squared_error':\n",
    "        for x, y in zip(validateX, validateY):\n",
    "            forward_propagation(number_of_layer, x.reshape(784, 1) / 255.0)\n",
    "            loss_local += squared_error(label=y, softmax_output=network[number_of_layer - 1]['h'])\n",
    "            max_prob = np.argmax(network[number_of_layer - 1]['h'])\n",
    "            if max_prob == y:\n",
    "                acc += 1\n",
    "    average_loss = loss_local / float(len(validateX))\n",
    "    acc = acc / float(len(validateX))\n",
    "    return [average_loss, acc]\n",
    "\n",
    "def fit(datapoints, batch, epochs, labels, opt, loss_type, augment):\n",
    "    \"\"\"\n",
    "    Train the neural network.\n",
    "\n",
    "    Args:\n",
    "        datapoints (numpy.ndarray): Input data.\n",
    "        batch (int): Batch size.\n",
    "        epochs (int): Number of epochs.\n",
    "        labels (numpy.ndarray): Labels.\n",
    "        opt (Optimizer): Optimization algorithm.\n",
    "        loss_type (str): Type of loss function.\n",
    "        augment (int): Number of augmented samples.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    n = len(network)  # Number of layers\n",
    "    d = len(datapoints)  # Number of data points\n",
    "    border = d - ((d - int(d * .1)) % batch + int(d * .1))\n",
    "    validateX = datapoints[border:]  # Validation data\n",
    "    validateY = labels[border:]  # Validation labels\n",
    "    datapoints = datapoints[:border]  # Training data\n",
    "    labels = labels[:border]  # Training labels\n",
    "    d = border\n",
    "    if augment is not None:\n",
    "        (datapoints, labels, d) = augment_my_data(datapoints=datapoints, labels=labels, d=d, newSize=d + augment * batch)\n",
    "    shuffler = np.arange(0, d)\n",
    "    for k in range(epochs):\n",
    "        np.random.shuffle(shuffler)\n",
    "        for i in range(0, d - batch + 1, batch):\n",
    "            clean = True\n",
    "            global loss\n",
    "            loss = 0\n",
    "            if isinstance(opt, NAG):\n",
    "                opt.lookahead(network=network)\n",
    "            for j in range(i, i + batch, 1):\n",
    "                x = datapoints[shuffler[j]].reshape(784, 1) / 255.0\n",
    "                y = labels[shuffler[j]]\n",
    "                forward_propagation(n, x)\n",
    "                backward_propagation(n, x, y, number_of_datapoint=batch, loss_type=loss_type, clean=clean)\n",
    "                clean = False\n",
    "            opt.descent(network=network, gradient=gradient)\n",
    "        validation_result = validate(number_of_layer=n, validateX=validateX, validateY=validateY,\n",
    "                                     loss_type=loss_type)\n",
    "        training_result = validate(number_of_layer=n, validateX=datapoints,\n",
    "                                   validateY=labels, loss_type=loss_type)\n",
    "        wandb.log({\"val_accuracy\": validation_result[1], 'val_loss': validation_result[0][0],\n",
    "                   'train_accuracy': training_result[1], 'train_loss': training_result[0][0], 'epochs': k + 1})\n",
    "        if np.isnan(validation_result[0])[0]:\n",
    "            return\n",
    "\n",
    "def add_layer(number_of_neurons, context, weight_init, input_dim=None):\n",
    "    \"\"\"\n",
    "    Add a layer to the neural network.\n",
    "\n",
    "    Args:\n",
    "        number_of_neurons (int): Number of neurons in the layer.\n",
    "        context (str): Activation function context.\n",
    "        weight_init (str): Weight initialization method.\n",
    "        input_dim (int, optional): Input dimension. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    layer = {}\n",
    "    if weight_init == 'random':\n",
    "        if input_dim is not None:\n",
    "            layer['weight'] = np.random.rand(number_of_neurons, input_dim)\n",
    "        else:\n",
    "            previous_lay_neuron_num = network[-1]['h'].shape[0]\n",
    "            layer['weight'] = np.random.rand(number_of_neurons, previous_lay_neuron_num)\n",
    "    elif weight_init == 'xavier':\n",
    "        if input_dim is not None:\n",
    "            layer['weight'] = np.random.normal(size=(number_of_neurons, input_dim))\n",
    "            xavier = input_dim\n",
    "        else:\n",
    "            previous_lay_neuron_num = network[-1]['h'].shape[0]\n",
    "            layer['weight'] = np.random.normal(size=(number_of_neurons, previous_lay_neuron_num))\n",
    "            xavier = previous_lay_neuron_num\n",
    "        if context == 'relu':\n",
    "            layer['weight'] = layer['weight'] * np.sqrt(2 / float(xavier))\n",
    "        else:\n",
    "            layer['weight'] = layer['weight'] * np.sqrt(1 / float(xavier))\n",
    "    layer['bias'] = np.zeros((number_of_neurons, 1))\n",
    "    layer['h'] = np.zeros((number_of_neurons, 1))\n",
    "    layer['a'] = np.zeros((number_of_neurons, 1))\n",
    "    layer['context'] = context\n",
    "    network.append(layer)\n",
    "\n",
    "# def master(batch, epochs, output_dim, activation, opt, layer_1, layer_2, layer_3, weight_init='xavier',loss_type='cross_entropy',\n",
    "#            augment=None):\n",
    "#     \"\"\"\n",
    "#     Initialize the neural network and start training.\n",
    "\n",
    "#     Args:\n",
    "#         batch (int): Batch size.\n",
    "#         epochs (int): Number of epochs.\n",
    "#         output_dim (int): Dimension of the output layer.\n",
    "#         activation (str): Activation function.\n",
    "#         opt (Optimizer): Optimization algorithm.\n",
    "#         layer_1 (int): Number of neurons in the first hidden layer.\n",
    "#         layer_2 (int): Number of neurons in the second hidden layer.\n",
    "#         layer_3 (int): Number of neurons in the third hidden layer.\n",
    "#         weight_init (str, optional): Weight initialization method. Defaults to 'xavier'.\n",
    "#         loss_type (str, optional): Type of loss function. Defaults to 'cross_entropy'.\n",
    "#         augment (int, optional): Number of augmented samples. Defaults to None.\n",
    "\n",
    "#     Returns:\n",
    "#         list: Learned parameters of the neural network.\n",
    "#     \"\"\"\n",
    "#     n_features = 784  # Number of input features per datapoint\n",
    "#     global network\n",
    "#     global gradient\n",
    "#     global transient_gradient\n",
    "#     network = []\n",
    "#     gradient = []\n",
    "#     transient_gradient = []\n",
    "#     add_layer(number_of_neurons=layer_1, context=activation, input_dim=784, weight_init=weight_init)\n",
    "#     add_layer(number_of_neurons=layer_2, context=activation, weight_init=weight_init)\n",
    "#     add_layer(number_of_neurons=layer_3, context=activation, weight_init=weight_init)\n",
    "#     add_layer(number_of_neurons=output_dim, context='softmax', weight_init=weight_init)\n",
    "#     gradient = copy.deepcopy(network)\n",
    "#     transient_gradient = copy.deepcopy(network)\n",
    "#     fit(datapoints=trainX, labels=trainY, batch=batch, epochs=epochs, opt=opt,\n",
    "#         loss_type=loss_type,augment=augment)\n",
    "#     return network\n",
    "\n",
    "def master(batch, epochs, output_dim, activation, opt, weight_init='xavier', loss_type='cross_entropy', augment=None, layer_neurons=[]):\n",
    "    \"\"\"\n",
    "    Initialize the neural network and start training.\n",
    "\n",
    "    Args:\n",
    "        batch (int): Batch size.\n",
    "        epochs (int): Number of epochs.\n",
    "        output_dim (int): Dimension of the output layer.\n",
    "        activation (str): Activation function.\n",
    "        opt (Optimizer): Optimization algorithm.\n",
    "        weight_init (str, optional): Weight initialization method. Defaults to 'xavier'.\n",
    "        loss_type (str, optional): Type of loss function. Defaults to 'cross_entropy'.\n",
    "        augment (int, optional): Number of augmented samples. Defaults to None.\n",
    "        layer_neurons (list): List of number of neurons in each hidden layer.\n",
    "\n",
    "    Returns:\n",
    "        list: Learned parameters of the neural network.\n",
    "    \"\"\"\n",
    "    n_features = 784  # Number of input features per datapoint\n",
    "    global network\n",
    "    global gradient\n",
    "    global transient_gradient\n",
    "    network = []\n",
    "    gradient = []\n",
    "    transient_gradient = []\n",
    "\n",
    "    # Adding input layer\n",
    "    add_layer(number_of_neurons=n_features, context=activation, input_dim=n_features, weight_init=weight_init)\n",
    "\n",
    "    # Adding hidden layers\n",
    "    for neurons in layer_neurons:\n",
    "        add_layer(number_of_neurons=neurons, context=activation, weight_init=weight_init)\n",
    "\n",
    "    # Adding output layer\n",
    "    add_layer(number_of_neurons=output_dim, context='softmax', weight_init=weight_init)\n",
    "\n",
    "    gradient = copy.deepcopy(network)\n",
    "    transient_gradient = copy.deepcopy(network)\n",
    "\n",
    "    fit(datapoints=trainX, labels=trainY, batch=batch, epochs=epochs, opt=opt,\n",
    "        loss_type=loss_type, augment=augment)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# def train():\n",
    "#     run = wandb.init()\n",
    "#     opti = None\n",
    "#     # Overwrite the random run names chosen by wandb\n",
    "#     name_str = f'act:{run.config.activation}_opt:{run.config.optimiser}_bs:{run.config.batch_size}_winit:{run.config.weight_init}_epoc:{run.config.epochs}_numlayers:{run.config.num_layers}_l1:{run.config.layer_1}_l2:{run.config.layer_2}_l3:{run.config.layer_3}_lr:{run.config.learning_rate}_wtDec:{run.config.weight_decay}'\n",
    "#     run.name = name_str\n",
    "\n",
    "#     if run.config.optimiser == 'nag':\n",
    "#         opti = NAG(layers=4, eta=run.config.learning_rate, gamma=.90, weight_decay=run.config.weight_decay)\n",
    "#     elif run.config.optimiser == 'rmsprop':\n",
    "#         opti = RMSProp(layers=4, eta=run.config.learning_rate, beta=.90, weight_decay=run.config.weight_decay)\n",
    "#     elif run.config.optimiser == 'sgd':\n",
    "#         opti = SimpleGradientDescent(layers=4, eta=run.config.learning_rate, weight_decay=run.config.weight_decay)\n",
    "#     elif run.config.optimiser == 'mom':\n",
    "#         opti = MomentumGradientDescent(layers=4, eta=run.config.learning_rate, gamma=.99,\n",
    "#                                        weight_decay=run.config.weight_decay)\n",
    "#     elif run.config.optimiser == 'adam':\n",
    "#         opti = ADAM(layers=4, eta=run.config.learning_rate, weight_decay=run.config.weight_decay)\n",
    "#     elif run.config.optimiser == 'nadam':\n",
    "#         opti = NADAM(layers=4, eta=run.config.learning_rate, weight_decay=run.config.weight_decay)\n",
    "\n",
    "#     master(epochs=run.config.epochs, batch=run.config.batch_size, output_dim=10,\n",
    "#            opt=opti, weight_init=run.config.weight_init, activation=run.config.activation, layer_1=run.config.layer_1,\n",
    "#            layer_3=run.config.layer_3, layer_2=run.config.layer_2, loss_type=run.config.loss_type, augment=100)\n",
    "\n",
    "#     wandb.finish()\n",
    "\n",
    "def train():\n",
    "    run = wandb.init()\n",
    "    opti = None\n",
    "\n",
    "    # Overwrite the random run names chosen by wandb\n",
    "    name_str = f'act:{run.config.activation}_opt:{run.config.optimiser}_bs:{run.config.batch_size}_winit:{run.config.weight_init}_epoc:{run.config.epochs}_numlayers:{len(run.config.layer_neurons)}'\n",
    "    for i, neurons in enumerate(run.config.layer_neurons, start=1):\n",
    "        name_str += f'_l{i}:{neurons}'\n",
    "    name_str += f'_lr:{run.config.learning_rate}_wtDec:{run.config.weight_decay}'\n",
    "    run.name = name_str\n",
    "\n",
    "    if run.config.optimiser == 'nag':\n",
    "        opti = NAG(layers=len(run.config.layer_neurons) + 1, eta=run.config.learning_rate, gamma=.90, weight_decay=run.config.weight_decay)\n",
    "    elif run.config.optimiser == 'rmsprop':\n",
    "        opti = RMSProp(layers=len(run.config.layer_neurons) + 1, eta=run.config.learning_rate, beta=.90, weight_decay=run.config.weight_decay)\n",
    "    elif run.config.optimiser == 'sgd':\n",
    "        opti = SimpleGradientDescent(layers=len(run.config.layer_neurons) + 1, eta=run.config.learning_rate, weight_decay=run.config.weight_decay)\n",
    "    elif run.config.optimiser == 'mom':\n",
    "        opti = MomentumGradientDescent(layers=len(run.config.layer_neurons) + 1, eta=run.config.learning_rate, gamma=.99,\n",
    "                                       weight_decay=run.config.weight_decay)\n",
    "    elif run.config.optimiser == 'adam':\n",
    "        opti = ADAM(layers=len(run.config.layer_neurons) + 1, eta=run.config.learning_rate, weight_decay=run.config.weight_decay)\n",
    "    elif run.config.optimiser == 'nadam':\n",
    "        opti = NADAM(layers=len(run.config.layer_neurons) + 1, eta=run.config.learning_rate, weight_decay=run.config.weight_decay)\n",
    "\n",
    "    master(epochs=run.config.epochs, batch=run.config.batch_size, output_dim=10,\n",
    "           opt=opti, weight_init=run.config.weight_init, activation=run.config.activation, layer_neurons=run.config.layer_neurons,\n",
    "           loss_type=run.config.loss_type, augment=100)\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e0079d",
   "metadata": {
    "id": "e8e0079d"
   },
   "outputs": [],
   "source": [
    "# # #############################################################################################################################\n",
    "# # # %%writefile save_model.py\n",
    "# # # from main import master, ADAM\n",
    "\n",
    "# import pickle\n",
    "# import wandb\n",
    "\n",
    "# # Initialize Wandb\n",
    "# wandb.init()\n",
    "\n",
    "# network = master(batch=495, epochs=7, output_dim=10, activation='tanh',\n",
    "#                  opt=ADAM(eta=0.003576466933615937, layers=4, weight_decay=0.31834976996809683),\n",
    "#                  layer_1=32, layer_2=64, layer_3=16, weight_init='xavier', loss_type='cross_entropy', augment=100)\n",
    "# print(len(network))\n",
    "# print(network)\n",
    "\n",
    "# filename_model = 'neural_network.object'\n",
    "# pickle.dump(network, open(filename_model, 'wb'))  # store best model's object to disk\n",
    "\n",
    "# # Finish Wandb run\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f9b11b",
   "metadata": {
    "id": "76f9b11b"
   },
   "outputs": [],
   "source": [
    "# ############################################################################################################################\n",
    "# # %%writefile confusion_matrix_plot.py\n",
    "# # Plot confusion matrix\n",
    "\n",
    "# import pickle\n",
    "# from keras.datasets import fashion_mnist\n",
    "# from sklearn import metrics\n",
    "# import numpy as np\n",
    "# import math\n",
    "# import seaborn as sn\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the trained network\n",
    "# filename_model = 'neural_network.object'\n",
    "# network = pickle.load(open(filename_model, 'rb'))\n",
    "\n",
    "# (trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
    "\n",
    "# def softmax(pre_activation_vector):\n",
    "#     post_act = np.copy(pre_activation_vector)\n",
    "#     max_exponent = np.max(post_act)\n",
    "#     post_act = np.exp(post_act - max_exponent)\n",
    "#     post_act = post_act / np.sum(post_act)\n",
    "#     return post_act\n",
    "\n",
    "# def relu(pre_activation_vector):\n",
    "#     post_act = np.copy(pre_activation_vector)\n",
    "#     post_act[post_act < 0] = 0\n",
    "#     return post_act\n",
    "\n",
    "# def sigmoid_element_wise(vector_component):\n",
    "#     if vector_component >= 0:\n",
    "#         return 1 / (1 + math.exp(-vector_component))\n",
    "#     else:\n",
    "#         return math.exp(vector_component) / (math.exp(vector_component) + 1)\n",
    "\n",
    "# def sigmoid(pre_activation_vector):\n",
    "#     activated_vector = np.empty_like(pre_activation_vector)\n",
    "#     for i, elem in np.ndenumerate(pre_activation_vector):\n",
    "#         activated_vector[i] = sigmoid_element_wise(elem)\n",
    "#     return activated_vector\n",
    "\n",
    "# def activation_function(pre_activation_vector, context):\n",
    "#     if context == 'softmax':\n",
    "#         return softmax(pre_activation_vector)\n",
    "#     elif context == 'sigmoid':\n",
    "#         return sigmoid(pre_activation_vector)\n",
    "#     elif context == 'tanh':\n",
    "#         return np.copy(np.tanh(pre_activation_vector))\n",
    "#     elif context == 'relu':\n",
    "#         return relu(pre_activation_vector)\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# def forward_propagation(n, x):\n",
    "#     for i in range(n):\n",
    "#         if i == 0:\n",
    "#             network[i]['a'] = network[i]['weight'] @ x + network[i]['bias']\n",
    "#         else:\n",
    "#             network[i]['a'] = network[i]['weight'] @ network[i - 1]['h'] + network[i]['bias']\n",
    "#         network[i]['h'] = activation_function(network[i]['a'], context=network[i]['context'])\n",
    "\n",
    "# cm_plot_labels = ['Top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# def predict_label(number_of_layer):\n",
    "#     acc = 0\n",
    "#     y_pred = []\n",
    "#     for x, y in zip(testX, testy):\n",
    "#         forward_propagation(number_of_layer, x.reshape(784, 1) / 255.0)\n",
    "#         max_prob = np.argmax(network[number_of_layer - 1]['h'])\n",
    "#         if max_prob == y:\n",
    "#             acc += 1\n",
    "#         y_pred.append(max_prob)\n",
    "#     print(\"Accuracy: \", str((acc / len(testy)) * 100), \"%\")\n",
    "#     cm = metrics.confusion_matrix(y_true=testy, y_pred=y_pred)\n",
    "#     df_cm = pd.DataFrame(cm, index=[i for i in cm_plot_labels], columns=[i for i in cm_plot_labels])\n",
    "#     print(df_cm)\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     ax = sn.heatmap(df_cm, annot=True, cmap='Blues', fmt='d', linewidths=3, linecolor='black')\n",
    "#     ax.set_yticklabels(cm_plot_labels, rotation=0)\n",
    "#     plt.xlabel(\"True Class\")\n",
    "#     plt.ylabel(\"Predicted Class\")\n",
    "#     plt.title('Confusion Matrix of FASHION-MNIST Dataset', fontsize=20)\n",
    "#     plt.show()\n",
    "\n",
    "# predict_label(len(network))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55b8b419",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "55b8b419",
    "outputId": "ae8bec45-56ae-44bb-aa19-cb92715f0a11"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\OM\\Desktop\\ABHIJEET\\IITM\\FoDL\\cs6910_a1_ee23d406\\wandb\\run-20240317_183347-Question-1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abhijeet001/Fashion-MNIST-Images/runs/Question-1' target=\"_blank\">Question-1</a></strong> to <a href='https://wandb.ai/abhijeet001/Fashion-MNIST-Images' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abhijeet001/Fashion-MNIST-Images' target=\"_blank\">https://wandb.ai/abhijeet001/Fashion-MNIST-Images</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abhijeet001/Fashion-MNIST-Images/runs/Question-1' target=\"_blank\">https://wandb.ai/abhijeet001/Fashion-MNIST-Images/runs/Question-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # ############################################################################################################################\n",
    "# # # %%writefile plot_images_fashion_mnist.py\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "import wandb\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"Fashion-MNIST-Images\", id=\"Question-1\")\n",
    "\n",
    "# Define class names for Fashion MNIST dataset\n",
    "class_names = ['T-shirt/top', 'Trouser/pants', 'Pullover shirt', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range 0-1\n",
    "trainX = trainX / 255.0\n",
    "testX = testX / 255.0\n",
    "\n",
    "def log_images():\n",
    "    # Initialize lists to store images and their corresponding labels\n",
    "    set_images = []\n",
    "    set_labels = []\n",
    "    count = 0  # Counter to keep track of how many images per class have been added\n",
    "    for d in range(len(trainy)):\n",
    "        if trainy[d] == count:\n",
    "            # Add the image and its label to the respective lists\n",
    "            set_images.append(trainX[d])\n",
    "            set_labels.append(class_names[trainy[d]])\n",
    "            count += 1\n",
    "        else:\n",
    "            pass\n",
    "        if count == 10:\n",
    "            break  # If images for all 10 classes have been collected, exit the loop\n",
    "\n",
    "    # Log the images and their labels to Weights & Biases\n",
    "    wandb.log({\"Plot\": [wandb.Image(img, caption=caption) for img, caption in zip(set_images, set_labels)]})\n",
    "\n",
    "# Call the function to log images\n",
    "log_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a417a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "89feea43ea0e4696842018ef28531c78",
      "813d3f2082ec43ddb8aaf05a2d6da429",
      "aee11f09703f4e01a4b17b713122720c",
      "a66cd0ce3be24472baf73f420175d597",
      "58dcc778515f40f0a7446fe03c0b8e41",
      "6828ce65b20a41e5a700ee51a4630998",
      "10f519e655c14182ba936048dbb2a6e2",
      "40b7e57e2b674987b1b3053de9d2cbf7",
      "73cfc9605c634a39aa345d6c8c387d3d",
      "76a3c5ec07114bfc8db8bb2eeb160534",
      "cf7dc96080be4d268ba6fd6714b9fc57",
      "6c212a2ee65440b38eb107e2a35f54de",
      "c97ed9cd6a1948fc9c09df3116c39119",
      "6d4336fd8c9b41beb75782c7d77a1a4a",
      "64cc2882a3a445d6aa2c648aa5c18530",
      "8b36b1d5fce443eda54d0970b82301dd",
      "b30216c23a8743a890af7847e10efebf",
      "a9f46cfef3814b72abc389114d5262b3",
      "5bf8bb51e3dc49319fc8ca11e7f2e69f",
      "9d1c8dfda8384ac7808b574fc829a635",
      "f1d2404065fc491798760600a6d7b8ea",
      "2dbec2bb4f3d47e290fb9ada5b3fa75a",
      "6d1ce18b769d4494ac31bda679d4472e",
      "4f5c19fdad4e47f89986dce3a008b89d"
     ]
    },
    "id": "670a417a",
    "outputId": "6792c6d1-b481-4e27-baa1-5de66ec1a7ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: hbou4x9k\n",
      "Sweep URL: https://wandb.ai/abhijeet001/CS6910_ASSIGNMENT_01/sweeps/hbou4x9k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7ikzv7ot with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_neurons: [64, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_type: cross_entropy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: random\n",
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "Exception in thread IntMsgThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "Exception in thread ChkStopThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\threading.py\", line 917, in run\n",
      "    self.run()\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\threading.py\", line 917, in run\n",
      "    self.run()\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 300, in check_internal_messages\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 268, in check_network_status\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 286, in check_stop_status\n",
      "    self._loop_check_status(\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 224, in _loop_check_status\n",
      "    self._loop_check_status(\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 224, in _loop_check_status\n",
      "    self._loop_check_status(\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 224, in _loop_check_status\n",
      "    local_handle = request()\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 803, in deliver_internal_messages\n",
      "    local_handle = request()\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 795, in deliver_network_status\n",
      "    local_handle = request()\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 787, in deliver_stop_status\n",
      "    return self._deliver_internal_messages(internal_message)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 506, in _deliver_internal_messages\n",
      "    return self._deliver_network_status(status)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 500, in _deliver_network_status\n",
      "    return self._deliver_stop_status(status)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 484, in _deliver_stop_status\n",
      "    return self._deliver_record(record)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 449, in _deliver_record\n",
      "    return self._deliver_record(record)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 449, in _deliver_record\n",
      "    return self._deliver_record(record)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 449, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 455, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 455, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 455, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "    interface._publish(record)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "    interface._publish(record)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n",
      "    self.send_server_request(server_req)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n",
      "    self.send_server_request(server_req)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n",
      "    self._send_message(msg)    \n",
      "self._send_message(msg)  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n",
      "\n",
      "      File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n",
      "self._sendall_with_error_handle(header + data)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "ConnectionAbortedError: [WinError 10053] An established connection was aborted by the software in your host machine\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"C:\\Users\\OM\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "ConnectionAbortedError: [WinError 10053] An established connection was aborted by the software in your host machine\n",
      "    sent = self._sock.send(data)\n",
      "ConnectionAbortedError: [WinError 10053] An established connection was aborted by the software in your host machine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\OM\\Desktop\\ABHIJEET\\IITM\\FoDL\\cs6910_a1_ee23d406\\wandb\\run-20240317_183418-7ikzv7ot</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abhijeet001/CS6910_ASSIGNMENT_01/runs/7ikzv7ot' target=\"_blank\">gallant-sweep-1</a></strong> to <a href='https://wandb.ai/abhijeet001/CS6910_ASSIGNMENT_01' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/abhijeet001/CS6910_ASSIGNMENT_01/sweeps/hbou4x9k' target=\"_blank\">https://wandb.ai/abhijeet001/CS6910_ASSIGNMENT_01/sweeps/hbou4x9k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abhijeet001/CS6910_ASSIGNMENT_01' target=\"_blank\">https://wandb.ai/abhijeet001/CS6910_ASSIGNMENT_01</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/abhijeet001/CS6910_ASSIGNMENT_01/sweeps/hbou4x9k' target=\"_blank\">https://wandb.ai/abhijeet001/CS6910_ASSIGNMENT_01/sweeps/hbou4x9k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abhijeet001/CS6910_ASSIGNMENT_01/runs/7ikzv7ot' target=\"_blank\">https://wandb.ai/abhijeet001/CS6910_ASSIGNMENT_01/runs/7ikzv7ot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Define the sweep configuration\n",
    "\n",
    "# sweep_config = {\n",
    "    \n",
    "#     \"name\": \"my_custom_sweep\",\n",
    "#     'method': 'grid',\n",
    "#     'parameters': {\n",
    "#         'epochs': {'values': [7, 10]},  # Add epochs parameter\n",
    "#         'num_layers': {'values': [3]},\n",
    "# #         'num_layers': {'values': [3, 4, 5]},\n",
    "      \n",
    "#         'layer_1': {'values': [64, 128]},\n",
    "#         'layer_2': {'values': [64, 128]},\n",
    "#         'layer_3': {'values': [64, 128]},\n",
    "# #         'layer_4': {'values': [32, 64, 128]},\n",
    "# #         'layer_5': {'values': [32, 64, 128]},\n",
    "#         'weight_decay': {'values': [0.0005, 0.5]},\n",
    "#         'learning_rate': {'values': [0.01, 0.001]},\n",
    "# #        'optimiser': {'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']},\n",
    "#         'optimiser': {'values': ['rmsprop', 'adam', 'nadam']},\n",
    "#         'batch_size': {'values': [16, 32, 64, 128, 256, 512]},\n",
    "#         'weight_init': {'values': ['random', 'xavier']},\n",
    "#         'activation': {'values': ['relu']},\n",
    "# #         'activation': {'values': ['sigmoid', 'tanh', 'relu']},\n",
    "#         'loss_type':{'values': ['cross_entropy', 'squared_error']}\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "# metric = {\n",
    "#     'name': 'val_accuracy',\n",
    "#     'goal': 'maximize'   \n",
    "#     }\n",
    "\n",
    "# sweep_config['metric'] = metric\n",
    "\n",
    "\n",
    "# # Initialize a sweep\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"CS6910_ASSIGNMENT_01\")\n",
    "\n",
    "# # Define the sweep function\n",
    "# # def sweep():\n",
    "# #     with wandb.init() as run:\n",
    "# #         config = run.config\n",
    "# #         train()\n",
    "\n",
    "# def sweep():\n",
    "#     train()\n",
    "\n",
    "# # Initialize the sweep agent\n",
    "\n",
    "# wandb.agent(sweep_id, function=sweep, count=50)\n",
    "\n",
    "# Define the sweep configuration\n",
    "sweep_config = {\n",
    "    \"name\": \"my_custom_sweep\",\n",
    "    'method': 'grid',\n",
    "    'parameters': {\n",
    "        'epochs': {'values': [7, 10]},  # Add epochs parameter\n",
    "        'layer_neurons': {'values': [ [32, 64, 16], [64, 64], [128, 128], [64, 128], [128, 64, 32]]},  # Modify to accept list of layer neurons\n",
    "        'weight_decay': {'values': [0.0005, 0.5]},\n",
    "        'learning_rate': {'values': [0.01, 0.001]},\n",
    "        'optimiser': {'values': ['rmsprop', 'adam', 'nadam']},\n",
    "        'batch_size': {'values': [16, 32, 64, 128, 256, 512]},\n",
    "        'weight_init': {'values': ['random', 'xavier']},\n",
    "        'activation': {'values': ['relu']},\n",
    "        'loss_type': {'values': ['cross_entropy', 'squared_error']}\n",
    "    }\n",
    "}\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_accuracy',\n",
    "    'goal': 'maximize'   \n",
    "}\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "# Initialize a sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_ASSIGNMENT_01\")\n",
    "\n",
    "# Define the sweep function\n",
    "def sweep():\n",
    "    train()\n",
    "\n",
    "# Initialize the sweep agent\n",
    "wandb.agent(sweep_id, function=sweep, count=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d16de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf44d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f515b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502987f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10f519e655c14182ba936048dbb2a6e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dbec2bb4f3d47e290fb9ada5b3fa75a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40b7e57e2b674987b1b3053de9d2cbf7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f5c19fdad4e47f89986dce3a008b89d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "58dcc778515f40f0a7446fe03c0b8e41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bf8bb51e3dc49319fc8ca11e7f2e69f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d1ce18b769d4494ac31bda679d4472e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4f5c19fdad4e47f89986dce3a008b89d",
      "value": 1
     }
    },
    "64cc2882a3a445d6aa2c648aa5c18530": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6828ce65b20a41e5a700ee51a4630998": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c212a2ee65440b38eb107e2a35f54de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d1ce18b769d4494ac31bda679d4472e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d4336fd8c9b41beb75782c7d77a1a4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73cfc9605c634a39aa345d6c8c387d3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_76a3c5ec07114bfc8db8bb2eeb160534",
       "IPY_MODEL_cf7dc96080be4d268ba6fd6714b9fc57"
      ],
      "layout": "IPY_MODEL_6c212a2ee65440b38eb107e2a35f54de"
     }
    },
    "76a3c5ec07114bfc8db8bb2eeb160534": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c97ed9cd6a1948fc9c09df3116c39119",
      "placeholder": "​",
      "style": "IPY_MODEL_6d4336fd8c9b41beb75782c7d77a1a4a",
      "value": "0.010 MB of 0.010 MB uploaded\r"
     }
    },
    "813d3f2082ec43ddb8aaf05a2d6da429": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58dcc778515f40f0a7446fe03c0b8e41",
      "placeholder": "​",
      "style": "IPY_MODEL_6828ce65b20a41e5a700ee51a4630998",
      "value": "0.010 MB of 0.010 MB uploaded\r"
     }
    },
    "89feea43ea0e4696842018ef28531c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_813d3f2082ec43ddb8aaf05a2d6da429",
       "IPY_MODEL_aee11f09703f4e01a4b17b713122720c"
      ],
      "layout": "IPY_MODEL_a66cd0ce3be24472baf73f420175d597"
     }
    },
    "8b36b1d5fce443eda54d0970b82301dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9d1c8dfda8384ac7808b574fc829a635": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a66cd0ce3be24472baf73f420175d597": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9f46cfef3814b72abc389114d5262b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1d2404065fc491798760600a6d7b8ea",
      "placeholder": "​",
      "style": "IPY_MODEL_2dbec2bb4f3d47e290fb9ada5b3fa75a",
      "value": "0.010 MB of 0.010 MB uploaded\r"
     }
    },
    "aee11f09703f4e01a4b17b713122720c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10f519e655c14182ba936048dbb2a6e2",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_40b7e57e2b674987b1b3053de9d2cbf7",
      "value": 1
     }
    },
    "b30216c23a8743a890af7847e10efebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a9f46cfef3814b72abc389114d5262b3",
       "IPY_MODEL_5bf8bb51e3dc49319fc8ca11e7f2e69f"
      ],
      "layout": "IPY_MODEL_9d1c8dfda8384ac7808b574fc829a635"
     }
    },
    "c97ed9cd6a1948fc9c09df3116c39119": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf7dc96080be4d268ba6fd6714b9fc57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64cc2882a3a445d6aa2c648aa5c18530",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b36b1d5fce443eda54d0970b82301dd",
      "value": 1
     }
    },
    "f1d2404065fc491798760600a6d7b8ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
